{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification with PyTorch and BERT\n",
    "\n",
    "This notebook demonstrates two distinct approaches for document classification:\n",
    "\n",
    "- A custom PyTorch-based model using pretrained Word2Vec embeddings.\n",
    "- A fine-tuned transformer model using DistilBERT from Huggingface.\n",
    "\n",
    "The dataset consists of humanitarian text excerpts, which are to be classified into sectors like health, protection, and education. This notebook covers the full pipeline: preprocessing, modeling, training, evaluation, and experiment tracking.\n",
    "\n",
    "Each section is self-contained, and comparisons are made between traditional and transformer-based approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Document Classification using PyTorch and Word2Vec\n",
    "\n",
    "In this section, I build a simple yet effective document classification model using PyTorch. Each document is represented by averaging the pretrained Word2Vec embeddings of its words. This average vector is then passed through a linear layer for classification.\n",
    "\n",
    "This approach is lightweight and fast, making it suitable for smaller models and quick experimentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T13:40:27.740157Z",
     "iopub.status.busy": "2024-01-07T13:40:27.739981Z",
     "iopub.status.idle": "2024-01-07T13:40:31.733309Z",
     "shell.execute_reply": "2024-01-07T13:40:31.732566Z",
     "shell.execute_reply.started": "2024-01-07T13:40:27.740141Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from unidecode import unidecode\n",
    "from collections import defaultdict\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.lm import Vocabulary\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import wandb\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T13:40:31.734978Z",
     "iopub.status.busy": "2024-01-07T13:40:31.734797Z",
     "iopub.status.idle": "2024-01-07T13:40:31.741803Z",
     "shell.execute_reply": "2024-01-07T13:40:31.741376Z",
     "shell.execute_reply.started": "2024-01-07T13:40:31.734961Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing(og_data: pd.DataFrame):\n",
    "    data = og_data.copy()\n",
    "    \n",
    "    print('Starting preprocessing... ', end='')\n",
    "    \n",
    "    # remove any non-words and non-spaces\n",
    "    data = data.replace(to_replace=r'[^\\w\\s]', value='', regex=True)\n",
    "    # replace numbers with placeholder\n",
    "    data = data.replace(to_replace=r'\\d+', value='num', regex=True)\n",
    "    # replace dates with placeholder\n",
    "    data = data.replace(to_replace=r'\\d+/\\d+/\\d+', value='dates', regex=True)\n",
    "    # remove accents, umlaute, etc\n",
    "    data['text'] = data['text'].apply(unidecode)\n",
    "    # tokenization\n",
    "    data['text'] = data['text'].apply(word_tokenize)\n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    data['text'] = data['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    # stemming\n",
    "    ps = PorterStemmer()\n",
    "    data['text'] = data['text'].apply(lambda x: [ps.stem(y) for y in x])\n",
    "    \n",
    "    print('Finished')\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def create_vocabulary(og_data: pd.DataFrame, unk_cutoff, vocabulary=None):\n",
    "    data = og_data.copy()\n",
    "    \n",
    "    if vocabulary is None:\n",
    "        print('Creating vocabulary... ', end='')\n",
    "        flat_list = [item for sublist in data['text'] for item in sublist]\n",
    "        vocabulary = Vocabulary(flat_list, unk_cutoff=unk_cutoff)\n",
    "        print('Finished')\n",
    "    \n",
    "    print('Applying vocabulary... ', end='')\n",
    "    data['text'] = data['text'].apply(lambda x: [word if word in vocabulary else '<UNK>' for word in x])\n",
    "    print('Finished')\n",
    "    \n",
    "    return vocabulary, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T13:40:31.742603Z",
     "iopub.status.busy": "2024-01-07T13:40:31.742439Z",
     "iopub.status.idle": "2024-01-07T13:40:31.913968Z",
     "shell.execute_reply": "2024-01-07T13:40:31.913445Z",
     "shell.execute_reply.started": "2024-01-07T13:40:31.742590Z"
    }
   },
   "outputs": [],
   "source": [
    "# Opening the training, validation and test set accordingly \n",
    "\n",
    "pattern = '\\d*,(.*)\\n'\n",
    "labels = []\n",
    "with open('data/thedeep.labels.txt') as f:\n",
    "    for l in f.readlines():\n",
    "        result = re.search(pattern, l)\n",
    "        labels.append(result.group(1))\n",
    "\n",
    "datasets = []\n",
    "\n",
    "for current_set in ('train', 'test', 'validation'):\n",
    "\n",
    "    sets = [[], [], []]\n",
    "    with open('data/thedeep.subset.' + current_set + '.txt', newline='') as csvf:\n",
    "        creader = csv.reader(csvf, delimiter=',')\n",
    "        for l in creader:\n",
    "            sets[2].append(int(l[0]))\n",
    "            sets[0].append(l[1].lower())\n",
    "            sets[1].append(int(l[2]))\n",
    "        current_dict = {'text': sets[0], 'labels': sets[1]}\n",
    "        datasets.append(pd.DataFrame(data=current_dict, index=sets[2]))\n",
    "\n",
    "train_data, test_data, val_data = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T13:40:31.914749Z",
     "iopub.status.busy": "2024-01-07T13:40:31.914560Z",
     "iopub.status.idle": "2024-01-07T13:40:45.918109Z",
     "shell.execute_reply": "2024-01-07T13:40:45.917493Z",
     "shell.execute_reply.started": "2024-01-07T13:40:31.914738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing... Finished\n",
      "Starting preprocessing... Finished\n",
      "Starting preprocessing... Finished\n",
      "Creating vocabulary... Finished\n",
      "Applying vocabulary... Finished\n",
      "Applying vocabulary... Finished\n",
      "Applying vocabulary... Finished\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "train_prep = preprocessing(train_data)\n",
    "test_prep = preprocessing(test_data)\n",
    "val_prep = preprocessing(val_data)\n",
    "\n",
    "# create dictionary, adapt data accordingly\n",
    "voc, train_v = create_vocabulary(train_prep, unk_cutoff=2)\n",
    "_, test_v = create_vocabulary(test_prep, unk_cutoff=2, vocabulary=voc)\n",
    "_, val_v = create_vocabulary(val_prep, unk_cutoff=2, vocabulary=voc)\n",
    "\n",
    "voc_dict = {word: i for i, word in enumerate(voc)}  # dictionary to get indices of words; usage: voc_dict[word] = index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T13:40:45.919110Z",
     "iopub.status.busy": "2024-01-07T13:40:45.918745Z",
     "iopub.status.idle": "2024-01-07T13:40:45.924831Z",
     "shell.execute_reply": "2024-01-07T13:40:45.924183Z",
     "shell.execute_reply.started": "2024-01-07T13:40:45.919098Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch(data_og:pd.DataFrame, vocabulary_dict: dict, batch_size:int, max_doc_len:int, shuffle=True, seed=None):\n",
    "\n",
    "    data = data_og.copy()\n",
    "    indices = np.array(data.index)\n",
    "\n",
    "    if shuffle == True:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        rng.shuffle(indices)\n",
    "\n",
    "    txt_batches = []\n",
    "    label_batches = []\n",
    "    \n",
    "    for i in range(0, len(indices), batch_size):\n",
    "        if i + batch_size > len(indices):  # cutoff last incomplete batch\n",
    "            break\n",
    "        else:\n",
    "            end = i + batch_size\n",
    "        \n",
    "        # get vocabulary ids, keep only first 'max_doc_len' words, pad document with 0 up to 'max_doc_len' if necessary, convert from pd.DataFrame to torch.Tensor\n",
    "        txt_batch = [np.pad(np.array([vocabulary_dict[word] for word in doc]), (0, max(0, max_doc_len - len(doc))))[:max_doc_len] for doc in data.loc[indices[i:end]]['text']]\n",
    "        label_batch = np.array(data.loc[indices[i:end]]['labels'])\n",
    "        \n",
    "        txt_batches.append(txt_batch)\n",
    "        label_batches.append(label_batch)\n",
    "\n",
    "    return torch.tensor(np.array(txt_batches), device=DEVICE), torch.tensor(np.array(label_batches), device=DEVICE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T15:14:58.578207Z",
     "iopub.status.busy": "2024-01-07T15:14:58.577502Z",
     "iopub.status.idle": "2024-01-07T15:14:59.175749Z",
     "shell.execute_reply": "2024-01-07T15:14:59.175229Z",
     "shell.execute_reply.started": "2024-01-07T15:14:58.578152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set shape:\ttorch.Size([189, 64, 50])\n",
      "test set shape:\t\ttorch.Size([40, 64, 50])\n",
      "validation set shape:\ttorch.Size([40, 64, 50])\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "MAX_DOC_LEN = 50\n",
    "SHUFFLE = True\n",
    "\n",
    "train_features, train_labels = batch(train_v, voc_dict, BATCH_SIZE, MAX_DOC_LEN, SHUFFLE)\n",
    "test_features, test_labels = batch(test_v, voc_dict, BATCH_SIZE, MAX_DOC_LEN, SHUFFLE)\n",
    "val_features, val_labels = batch(val_v, voc_dict, BATCH_SIZE, MAX_DOC_LEN, SHUFFLE)\n",
    "\n",
    "print(f'train set shape:\\t{train_features.shape}')\n",
    "print(f'test set shape:\\t\\t{test_features.shape}')\n",
    "print(f'validation set shape:\\t{val_features.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T14:54:15.886649Z",
     "iopub.status.busy": "2024-01-07T14:54:15.886499Z",
     "iopub.status.idle": "2024-01-07T14:54:16.443328Z",
     "shell.execute_reply": "2024-01-07T14:54:16.442783Z",
     "shell.execute_reply.started": "2024-01-07T14:54:15.886637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "source": [
    "# add PCA representation of training set to tensorboard\n",
    "\n",
    "writer = SummaryWriter('runs/projection')  # initialize tensorboard\n",
    "\n",
    "class_labels = [labels[lab] for lab in train_labels.flatten()]\n",
    "log_features = torch.flatten(train_features, start_dim=0, end_dim=1)\n",
    "\n",
    "writer.add_embedding(log_features, metadata=class_labels)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T13:40:48.401772Z",
     "iopub.status.busy": "2024-01-07T13:40:48.401627Z",
     "iopub.status.idle": "2024-01-07T13:41:28.680775Z",
     "shell.execute_reply": "2024-01-07T13:41:28.680193Z",
     "shell.execute_reply.started": "2024-01-07T13:40:48.401760Z"
    }
   },
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')  # load Word2Vec trained on Google News dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T14:54:23.008563Z",
     "iopub.status.busy": "2024-01-07T14:54:23.007872Z",
     "iopub.status.idle": "2024-01-07T14:54:26.222216Z",
     "shell.execute_reply": "2024-01-07T14:54:26.221645Z",
     "shell.execute_reply.started": "2024-01-07T14:54:23.008509Z"
    }
   },
   "outputs": [],
   "source": [
    "dict_size = len(voc)\n",
    "embed_size = wv.vector_size\n",
    "wv_mean = np.mean(wv.vectors)\n",
    "wv_std = np.std(wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T14:54:26.223144Z",
     "iopub.status.busy": "2024-01-07T14:54:26.222987Z",
     "iopub.status.idle": "2024-01-07T14:54:26.548400Z",
     "shell.execute_reply": "2024-01-07T14:54:26.547834Z",
     "shell.execute_reply.started": "2024-01-07T14:54:26.223132Z"
    }
   },
   "outputs": [],
   "source": [
    "weight = torch.empty(size=(dict_size, embed_size), dtype=torch.float, device=DEVICE)\n",
    "\n",
    "for i, word in enumerate(voc):\n",
    "    if word in wv:  # current word is present in Word2Vec\n",
    "        current_rep = np.copy(wv[word])\n",
    "        weight[i] = torch.FloatTensor(current_rep)\n",
    "    else:  # current word is not present in Word2Vec\n",
    "        random_rep = torch.normal(wv_mean, wv_std, size=(1, embed_size))\n",
    "        weight[i] = random_rep\n",
    "\n",
    "embedding = nn.Embedding.from_pretrained(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T14:54:26.549072Z",
     "iopub.status.busy": "2024-01-07T14:54:26.548862Z",
     "iopub.status.idle": "2024-01-07T14:54:26.554053Z",
     "shell.execute_reply": "2024-01-07T14:54:26.553544Z",
     "shell.execute_reply.started": "2024-01-07T14:54:26.549061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## test word: support ##\n",
      "word correctly embedded\n",
      "## test word: malaria ##\n",
      "word correctly embedded\n",
      "## test word: easter ##\n",
      "word correctly embedded\n"
     ]
    }
   ],
   "source": [
    "# check that embeddings are correct\n",
    "\n",
    "test_words = ['support', 'malaria', 'easter']\n",
    "\n",
    "for word in test_words:\n",
    "    word_idx = voc_dict[word]\n",
    "    embed_rep = embedding(torch.tensor(word_idx, device=DEVICE))\n",
    "    wv_rep = wv[word]\n",
    "    print(f'## test word: {word} ##')\n",
    "    if torch.sum(embed_rep.cpu() - wv_rep) == 0:\n",
    "        print('word correctly embedded')\n",
    "    else:\n",
    "        print('false embedding!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T13:48:42.357336Z",
     "iopub.status.busy": "2024-01-07T13:48:42.356610Z",
     "iopub.status.idle": "2024-01-07T13:48:42.378533Z",
     "shell.execute_reply": "2024-01-07T13:48:42.377048Z",
     "shell.execute_reply.started": "2024-01-07T13:48:42.357280Z"
    }
   },
   "outputs": [],
   "source": [
    "class ClassificationAverageModel(nn.Module):\n",
    "    def __init__(self, embedding_layer, in_features, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = embedding_layer\n",
    "        self.linear = nn.Linear(in_features, n_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.mean(self.embedding(x), axis=1)\n",
    "        output = self.softmax(self.linear(x))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T15:16:10.753422Z",
     "iopub.status.busy": "2024-01-07T15:16:10.752998Z",
     "iopub.status.idle": "2024-01-07T15:16:10.778643Z",
     "shell.execute_reply": "2024-01-07T15:16:10.778050Z",
     "shell.execute_reply.started": "2024-01-07T15:16:10.753390Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(logits, labels):\n",
    "    \n",
    "    idx = np.argmax(logits, axis=1)\n",
    "    return np.mean(labels[np.arange(len(idx)), idx])\n",
    "\n",
    "def to_one_hot(y, k=None):\n",
    "    if type(y) == torch.Tensor:\n",
    "        y = y.cpu()\n",
    "    y = np.asarray(y, dtype='int')\n",
    "    if k is None:\n",
    "        k = np.amax(y) + 1\n",
    "\n",
    "    out = np.zeros(y.shape + (k, ))\n",
    "    np.put_along_axis(out, y[..., None], 1, axis=-1)\n",
    "    return torch.Tensor(out)\n",
    "    \n",
    "\n",
    "def train(train_loader, val_loader, model, loss, optimizer, n_classes, epochs, device, n_early_stop, model_name='model'):\n",
    "\n",
    "    epoch_losses = []\n",
    "    epoch_accuracies = []\n",
    "    best_accuracy = 0\n",
    "    n_not_improved = 0\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    dataiter = iter(train_loader)\n",
    "    batch_size = next(dataiter)[0].shape[1]\n",
    "    doc_len = next(dataiter)[0].shape[2]\n",
    "\n",
    "    writer = SummaryWriter('runs/train_nepochs_' + str(epochs) + '_lr_' + str(lr) + '_batchsz_' + str(batch_size) + '_doclen_' + str(doc_len))  # initialize tensorboard\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}]\\n--------------------------------')\n",
    "        print('########### Training ###########')\n",
    "        \n",
    "        model.train()\n",
    "        current_losses = []\n",
    "        \n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            x = torch.squeeze(x).to(device)\n",
    "            y = torch.squeeze(to_one_hot(y, 12)).to(device)\n",
    "            \n",
    "            logits = model.forward(x)\n",
    "            batch_loss = loss(logits, y)\n",
    "            current_losses.append(batch_loss.detach().cpu().numpy())\n",
    "            writer.add_scalar(\"Loss/training set\", batch_loss, epoch * len(train_loader) + i)\n",
    "            \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i + 1) % 1500 == 0 or i+1 == len(train_loader) or i == 0:\n",
    "                print(f\"Step [{str(i + 1).zfill(len(str(len(train_loader))))}/{len(train_loader)}]\", end='')\n",
    "                print(\"  Loss: {:.4f}\".format(np.sum(current_losses) / len(current_losses)))\n",
    "\n",
    "        epoch_losses.append(current_losses)\n",
    "\n",
    "        print('########## Validation ##########')\n",
    "        model.eval()\n",
    "        current_accuracies = []\n",
    "\n",
    "        for i, (x, y) in enumerate(val_loader):\n",
    "            x = torch.squeeze(x).to(device)\n",
    "            y = torch.squeeze(to_one_hot(y, n_classes)).to(device)\n",
    "\n",
    "            logits = model.forward(x)\n",
    "            current_accuracies.append(accuracy(logits.detach().cpu().numpy(), y.detach().cpu().numpy()))\n",
    "\n",
    "        epoch_accuracies.append(current_accuracies)\n",
    "        avg_accuracy = np.sum(current_accuracies) / len(current_accuracies)\n",
    "        writer.add_scalar(\"Accuracy/validation\", avg_accuracy, epoch)\n",
    "\n",
    "        if avg_accuracy > best_accuracy:  # save model if accuracy is best one yet\n",
    "            n_not_improved = 0\n",
    "            best_accuracy = avg_accuracy\n",
    "            if not os.path.exists('./model_saves'):\n",
    "                os.mkdir('./model_saves')\n",
    "            torch.save(model.state_dict(), f'./model_saves/{model_name}.pt')\n",
    "\n",
    "        else:\n",
    "            n_not_improved += 1\n",
    "        \n",
    "        print('Average accuracy over validation set:  {:.4f}\\n'.format((avg_accuracy)))  \n",
    "        if n_not_improved == n_early_stop:\n",
    "            print('Early stopping initiated!')\n",
    "            break\n",
    "\n",
    "    return epoch_losses, epoch_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T15:16:11.473900Z",
     "iopub.status.busy": "2024-01-07T15:16:11.473281Z",
     "iopub.status.idle": "2024-01-07T15:16:11.482258Z",
     "shell.execute_reply": "2024-01-07T15:16:11.481230Z",
     "shell.execute_reply.started": "2024-01-07T15:16:11.473853Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "LR = 0.01\n",
    "EPOCHS = 1000\n",
    "N_EARLY_STOPS = 50\n",
    "\n",
    "train_set = TensorDataset(train_features, train_labels)\n",
    "test_set = TensorDataset(test_features, test_labels)\n",
    "val_set = TensorDataset(val_features, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_set)\n",
    "test_loader = DataLoader(test_set)\n",
    "val_loader = DataLoader(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T15:16:11.989761Z",
     "iopub.status.busy": "2024-01-07T15:16:11.989500Z",
     "iopub.status.idle": "2024-01-07T15:17:34.054742Z",
     "shell.execute_reply": "2024-01-07T15:17:34.054325Z",
     "shell.execute_reply.started": "2024-01-07T15:16:11.989749Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 2.4840\n",
      "Step [189/189]  Loss: 2.2696\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.5023\n",
      "\n",
      "Epoch [2/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 2.1864\n",
      "Step [189/189]  Loss: 2.1633\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.5395\n",
      "\n",
      "Epoch [3/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 2.1254\n",
      "Step [189/189]  Loss: 2.1306\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.5473\n",
      "\n",
      "Epoch [4/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 2.0987\n",
      "Step [189/189]  Loss: 2.1135\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.5504\n",
      "\n",
      "Epoch [5/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 2.0839\n",
      "Step [189/189]  Loss: 2.1025\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.5531\n",
      "\n",
      "Epoch [6/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 2.0746\n",
      "Step [189/189]  Loss: 2.0947\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.5551\n",
      "\n",
      "Epoch [7/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 2.0682\n",
      "Step [189/189]  Loss: 2.0888\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.5563\n",
      "\n",
      "Epoch [8/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 2.0636\n",
      "Step [189/189]  Loss: 2.0841\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.5570\n",
      "\n",
      "Epoch [9/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 2.0601\n",
      "Step [189/189]  Loss: 2.0758\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6039\n",
      "\n",
      "Epoch [10/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 2.0491\n",
      "Step [189/189]  Loss: 2.0560\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6086\n",
      "\n",
      "Epoch [11/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 2.0370\n",
      "Step [189/189]  Loss: 2.0489\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6133\n",
      "\n",
      "Epoch [12/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 2.0313\n",
      "Step [189/189]  Loss: 2.0444\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6125\n",
      "\n",
      "Epoch [13/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 2.0271\n",
      "Step [189/189]  Loss: 2.0408\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6133\n",
      "\n",
      "Epoch [14/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 2.0237\n",
      "Step [189/189]  Loss: 2.0379\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6148\n",
      "\n",
      "Epoch [15/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 2.0208\n",
      "Step [189/189]  Loss: 2.0354\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6156\n",
      "\n",
      "Epoch [16/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 2.0182\n",
      "Step [189/189]  Loss: 2.0331\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6156\n",
      "\n",
      "Epoch [17/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 2.0160\n",
      "Step [189/189]  Loss: 2.0312\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6172\n",
      "\n",
      "Epoch [18/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 2.0140\n",
      "Step [189/189]  Loss: 2.0294\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6176\n",
      "\n",
      "Epoch [19/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 2.0122\n",
      "Step [189/189]  Loss: 2.0269\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6242\n",
      "\n",
      "Epoch [20/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 2.0047\n",
      "Step [189/189]  Loss: 2.0139\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6371\n",
      "\n",
      "Epoch [21/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9974\n",
      "Step [189/189]  Loss: 2.0042\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6453\n",
      "\n",
      "Epoch [22/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9916\n",
      "Step [189/189]  Loss: 1.9995\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6504\n",
      "\n",
      "Epoch [23/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9868\n",
      "Step [189/189]  Loss: 1.9962\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6523\n",
      "\n",
      "Epoch [24/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9827\n",
      "Step [189/189]  Loss: 1.9935\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6527\n",
      "\n",
      "Epoch [25/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9792\n",
      "Step [189/189]  Loss: 1.9913\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6531\n",
      "\n",
      "Epoch [26/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9761\n",
      "Step [189/189]  Loss: 1.9893\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6555\n",
      "\n",
      "Epoch [27/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9734\n",
      "Step [189/189]  Loss: 1.9875\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6555\n",
      "\n",
      "Epoch [28/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9710\n",
      "Step [189/189]  Loss: 1.9859\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6551\n",
      "\n",
      "Epoch [29/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9688\n",
      "Step [189/189]  Loss: 1.9844\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6555\n",
      "\n",
      "Epoch [30/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9668\n",
      "Step [189/189]  Loss: 1.9830\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6547\n",
      "\n",
      "Epoch [31/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9650\n",
      "Step [189/189]  Loss: 1.9724\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6707\n",
      "\n",
      "Epoch [32/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9367\n",
      "Step [189/189]  Loss: 1.9646\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6730\n",
      "\n",
      "Epoch [33/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9300\n",
      "Step [189/189]  Loss: 1.9615\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6754\n",
      "\n",
      "Epoch [34/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9263\n",
      "Step [189/189]  Loss: 1.9592\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6754\n",
      "\n",
      "Epoch [35/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9236\n",
      "Step [189/189]  Loss: 1.9573\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6766\n",
      "\n",
      "Epoch [36/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9215\n",
      "Step [189/189]  Loss: 1.9556\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6770\n",
      "\n",
      "Epoch [37/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9197\n",
      "Step [189/189]  Loss: 1.9542\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6777\n",
      "\n",
      "Epoch [38/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9181\n",
      "Step [189/189]  Loss: 1.9528\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6793\n",
      "\n",
      "Epoch [39/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9167\n",
      "Step [189/189]  Loss: 1.9515\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6813\n",
      "\n",
      "Epoch [40/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9154\n",
      "Step [189/189]  Loss: 1.9503\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6809\n",
      "\n",
      "Epoch [41/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9142\n",
      "Step [189/189]  Loss: 1.9492\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6828\n",
      "\n",
      "Epoch [42/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9131\n",
      "Step [189/189]  Loss: 1.9482\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6832\n",
      "\n",
      "Epoch [43/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9121\n",
      "Step [189/189]  Loss: 1.9472\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6840\n",
      "\n",
      "Epoch [44/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9111\n",
      "Step [189/189]  Loss: 1.9462\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6844\n",
      "\n",
      "Epoch [45/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9102\n",
      "Step [189/189]  Loss: 1.9453\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6852\n",
      "\n",
      "Epoch [46/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9094\n",
      "Step [189/189]  Loss: 1.9444\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6848\n",
      "\n",
      "Epoch [47/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9085\n",
      "Step [189/189]  Loss: 1.9436\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6852\n",
      "\n",
      "Epoch [48/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9077\n",
      "Step [189/189]  Loss: 1.9428\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6852\n",
      "\n",
      "Epoch [49/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9070\n",
      "Step [189/189]  Loss: 1.9420\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6859\n",
      "\n",
      "Epoch [50/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9062\n",
      "Step [189/189]  Loss: 1.9412\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6863\n",
      "\n",
      "Epoch [51/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9055\n",
      "Step [189/189]  Loss: 1.9405\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6859\n",
      "\n",
      "Epoch [52/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9048\n",
      "Step [189/189]  Loss: 1.9398\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.6871\n",
      "\n",
      "Epoch [53/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.9042\n",
      "Step [189/189]  Loss: 1.9355\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7074\n",
      "\n",
      "Epoch [54/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8906\n",
      "Step [189/189]  Loss: 1.9202\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7168\n",
      "\n",
      "Epoch [55/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8721\n",
      "Step [189/189]  Loss: 1.9157\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7207\n",
      "\n",
      "Epoch [56/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8629\n",
      "Step [189/189]  Loss: 1.9133\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7227\n",
      "\n",
      "Epoch [57/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8568\n",
      "Step [189/189]  Loss: 1.9116\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7230\n",
      "\n",
      "Epoch [58/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8530\n",
      "Step [189/189]  Loss: 1.9101\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7242\n",
      "\n",
      "Epoch [59/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8504\n",
      "Step [189/189]  Loss: 1.9089\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7258\n",
      "\n",
      "Epoch [60/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8485\n",
      "Step [189/189]  Loss: 1.9078\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7258\n",
      "\n",
      "Epoch [61/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8469\n",
      "Step [189/189]  Loss: 1.9067\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7258\n",
      "\n",
      "Epoch [62/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8456\n",
      "Step [189/189]  Loss: 1.9058\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7250\n",
      "\n",
      "Epoch [63/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8444\n",
      "Step [189/189]  Loss: 1.9049\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7258\n",
      "\n",
      "Epoch [64/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8434\n",
      "Step [189/189]  Loss: 1.9041\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7270\n",
      "\n",
      "Epoch [65/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8424\n",
      "Step [189/189]  Loss: 1.9033\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7266\n",
      "\n",
      "Epoch [66/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8416\n",
      "Step [189/189]  Loss: 1.9025\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7270\n",
      "\n",
      "Epoch [67/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8408\n",
      "Step [189/189]  Loss: 1.9018\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7277\n",
      "\n",
      "Epoch [68/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8400\n",
      "Step [189/189]  Loss: 1.9011\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7277\n",
      "\n",
      "Epoch [69/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8393\n",
      "Step [189/189]  Loss: 1.9004\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7285\n",
      "\n",
      "Epoch [70/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8386\n",
      "Step [189/189]  Loss: 1.8997\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7285\n",
      "\n",
      "Epoch [71/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8380\n",
      "Step [189/189]  Loss: 1.8991\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7293\n",
      "\n",
      "Epoch [72/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8374\n",
      "Step [189/189]  Loss: 1.8985\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7297\n",
      "\n",
      "Epoch [73/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8368\n",
      "Step [189/189]  Loss: 1.8979\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7297\n",
      "\n",
      "Epoch [74/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8362\n",
      "Step [189/189]  Loss: 1.8973\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7297\n",
      "\n",
      "Epoch [75/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8356\n",
      "Step [189/189]  Loss: 1.8968\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7301\n",
      "\n",
      "Epoch [76/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8351\n",
      "Step [189/189]  Loss: 1.8962\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [77/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8346\n",
      "Step [189/189]  Loss: 1.8957\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [78/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8341\n",
      "Step [189/189]  Loss: 1.8952\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [79/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8335\n",
      "Step [189/189]  Loss: 1.8947\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7301\n",
      "\n",
      "Epoch [80/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8331\n",
      "Step [189/189]  Loss: 1.8942\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7301\n",
      "\n",
      "Epoch [81/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8326\n",
      "Step [189/189]  Loss: 1.8937\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [82/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8321\n",
      "Step [189/189]  Loss: 1.8932\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [83/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8316\n",
      "Step [189/189]  Loss: 1.8927\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7301\n",
      "\n",
      "Epoch [84/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8312\n",
      "Step [189/189]  Loss: 1.8923\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7297\n",
      "\n",
      "Epoch [85/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8307\n",
      "Step [189/189]  Loss: 1.8918\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7297\n",
      "\n",
      "Epoch [86/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8303\n",
      "Step [189/189]  Loss: 1.8914\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7297\n",
      "\n",
      "Epoch [87/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8298\n",
      "Step [189/189]  Loss: 1.8910\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7301\n",
      "\n",
      "Epoch [88/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8294\n",
      "Step [189/189]  Loss: 1.8906\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7301\n",
      "\n",
      "Epoch [89/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8290\n",
      "Step [189/189]  Loss: 1.8902\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7301\n",
      "\n",
      "Epoch [90/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8286\n",
      "Step [189/189]  Loss: 1.8898\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7301\n",
      "\n",
      "Epoch [91/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8282\n",
      "Step [189/189]  Loss: 1.8894\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7301\n",
      "\n",
      "Epoch [92/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8278\n",
      "Step [189/189]  Loss: 1.8890\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7301\n",
      "\n",
      "Epoch [93/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8274\n",
      "Step [189/189]  Loss: 1.8886\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7301\n",
      "\n",
      "Epoch [94/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8270\n",
      "Step [189/189]  Loss: 1.8882\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [95/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8266\n",
      "Step [189/189]  Loss: 1.8879\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [96/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8263\n",
      "Step [189/189]  Loss: 1.8875\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7312\n",
      "\n",
      "Epoch [97/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8259\n",
      "Step [189/189]  Loss: 1.8871\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7312\n",
      "\n",
      "Epoch [98/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8256\n",
      "Step [189/189]  Loss: 1.8868\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7312\n",
      "\n",
      "Epoch [99/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8252\n",
      "Step [189/189]  Loss: 1.8865\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7312\n",
      "\n",
      "Epoch [100/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8249\n",
      "Step [189/189]  Loss: 1.8861\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7312\n",
      "\n",
      "Epoch [101/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8246\n",
      "Step [189/189]  Loss: 1.8858\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [102/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8243\n",
      "Step [189/189]  Loss: 1.8855\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [103/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8240\n",
      "Step [189/189]  Loss: 1.8851\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [104/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8237\n",
      "Step [189/189]  Loss: 1.8848\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [105/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8234\n",
      "Step [189/189]  Loss: 1.8845\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [106/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8231\n",
      "Step [189/189]  Loss: 1.8842\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [107/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8228\n",
      "Step [189/189]  Loss: 1.8839\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [108/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8225\n",
      "Step [189/189]  Loss: 1.8836\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [109/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8223\n",
      "Step [189/189]  Loss: 1.8833\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [110/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8220\n",
      "Step [189/189]  Loss: 1.8830\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [111/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8217\n",
      "Step [189/189]  Loss: 1.8828\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [112/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8215\n",
      "Step [189/189]  Loss: 1.8825\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [113/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8213\n",
      "Step [189/189]  Loss: 1.8822\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [114/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8210\n",
      "Step [189/189]  Loss: 1.8819\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [115/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8208\n",
      "Step [189/189]  Loss: 1.8817\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7301\n",
      "\n",
      "Epoch [116/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8206\n",
      "Step [189/189]  Loss: 1.8814\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7297\n",
      "\n",
      "Epoch [117/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8203\n",
      "Step [189/189]  Loss: 1.8811\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7297\n",
      "\n",
      "Epoch [118/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8201\n",
      "Step [189/189]  Loss: 1.8809\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7297\n",
      "\n",
      "Epoch [119/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8199\n",
      "Step [189/189]  Loss: 1.8806\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7301\n",
      "\n",
      "Epoch [120/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8197\n",
      "Step [189/189]  Loss: 1.8804\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [121/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8195\n",
      "Step [189/189]  Loss: 1.8801\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [122/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8193\n",
      "Step [189/189]  Loss: 1.8799\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [123/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8191\n",
      "Step [189/189]  Loss: 1.8796\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [124/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8189\n",
      "Step [189/189]  Loss: 1.8794\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [125/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8187\n",
      "Step [189/189]  Loss: 1.8792\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [126/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8185\n",
      "Step [189/189]  Loss: 1.8789\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [127/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8183\n",
      "Step [189/189]  Loss: 1.8787\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [128/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8181\n",
      "Step [189/189]  Loss: 1.8785\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [129/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8179\n",
      "Step [189/189]  Loss: 1.8782\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [130/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8177\n",
      "Step [189/189]  Loss: 1.8780\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [131/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8175\n",
      "Step [189/189]  Loss: 1.8778\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7301\n",
      "\n",
      "Epoch [132/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8173\n",
      "Step [189/189]  Loss: 1.8776\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7301\n",
      "\n",
      "Epoch [133/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8172\n",
      "Step [189/189]  Loss: 1.8773\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7301\n",
      "\n",
      "Epoch [134/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8170\n",
      "Step [189/189]  Loss: 1.8771\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7301\n",
      "\n",
      "Epoch [135/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8168\n",
      "Step [189/189]  Loss: 1.8769\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7301\n",
      "\n",
      "Epoch [136/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8166\n",
      "Step [189/189]  Loss: 1.8767\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7297\n",
      "\n",
      "Epoch [137/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8164\n",
      "Step [189/189]  Loss: 1.8765\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7297\n",
      "\n",
      "Epoch [138/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8163\n",
      "Step [189/189]  Loss: 1.8763\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7301\n",
      "\n",
      "Epoch [139/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8161\n",
      "Step [189/189]  Loss: 1.8761\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [140/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8159\n",
      "Step [189/189]  Loss: 1.8759\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [141/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8158\n",
      "Step [189/189]  Loss: 1.8757\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [142/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8156\n",
      "Step [189/189]  Loss: 1.8755\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [143/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8154\n",
      "Step [189/189]  Loss: 1.8753\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [144/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8153\n",
      "Step [189/189]  Loss: 1.8751\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [145/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8151\n",
      "Step [189/189]  Loss: 1.8749\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7312\n",
      "\n",
      "Epoch [146/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8150\n",
      "Step [189/189]  Loss: 1.8747\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7316\n",
      "\n",
      "Epoch [147/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8148\n",
      "Step [189/189]  Loss: 1.8745\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7312\n",
      "\n",
      "Epoch [148/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8147\n",
      "Step [189/189]  Loss: 1.8743\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7312\n",
      "\n",
      "Epoch [149/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8145\n",
      "Step [189/189]  Loss: 1.8742\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7312\n",
      "\n",
      "Epoch [150/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8143\n",
      "Step [189/189]  Loss: 1.8740\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [151/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8142\n",
      "Step [189/189]  Loss: 1.8738\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [152/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8140\n",
      "Step [189/189]  Loss: 1.8736\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [153/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8139\n",
      "Step [189/189]  Loss: 1.8734\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [154/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8138\n",
      "Step [189/189]  Loss: 1.8733\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [155/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8136\n",
      "Step [189/189]  Loss: 1.8731\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [156/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8135\n",
      "Step [189/189]  Loss: 1.8729\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7312\n",
      "\n",
      "Epoch [157/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8133\n",
      "Step [189/189]  Loss: 1.8727\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7312\n",
      "\n",
      "Epoch [158/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8132\n",
      "Step [189/189]  Loss: 1.8726\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7316\n",
      "\n",
      "Epoch [159/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8130\n",
      "Step [189/189]  Loss: 1.8724\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7316\n",
      "\n",
      "Epoch [160/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8129\n",
      "Step [189/189]  Loss: 1.8722\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7316\n",
      "\n",
      "Epoch [161/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8128\n",
      "Step [189/189]  Loss: 1.8721\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7316\n",
      "\n",
      "Epoch [162/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8126\n",
      "Step [189/189]  Loss: 1.8719\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7316\n",
      "\n",
      "Epoch [163/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8125\n",
      "Step [189/189]  Loss: 1.8718\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7316\n",
      "\n",
      "Epoch [164/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8124\n",
      "Step [189/189]  Loss: 1.8716\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7312\n",
      "\n",
      "Epoch [165/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8122\n",
      "Step [189/189]  Loss: 1.8714\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [166/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8121\n",
      "Step [189/189]  Loss: 1.8713\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [167/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8120\n",
      "Step [189/189]  Loss: 1.8711\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [168/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8118\n",
      "Step [189/189]  Loss: 1.8710\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [169/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8117\n",
      "Step [189/189]  Loss: 1.8708\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [170/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8116\n",
      "Step [189/189]  Loss: 1.8707\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [171/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8115\n",
      "Step [189/189]  Loss: 1.8705\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7301\n",
      "\n",
      "Epoch [172/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8113\n",
      "Step [189/189]  Loss: 1.8704\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7301\n",
      "\n",
      "Epoch [173/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8112\n",
      "Step [189/189]  Loss: 1.8702\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7297\n",
      "\n",
      "Epoch [174/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8111\n",
      "Step [189/189]  Loss: 1.8701\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7301\n",
      "\n",
      "Epoch [175/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8110\n",
      "Step [189/189]  Loss: 1.8699\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [176/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8109\n",
      "Step [189/189]  Loss: 1.8698\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [177/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8107\n",
      "Step [189/189]  Loss: 1.8696\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [178/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8106\n",
      "Step [189/189]  Loss: 1.8695\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [179/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8105\n",
      "Step [189/189]  Loss: 1.8694\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [180/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8104\n",
      "Step [189/189]  Loss: 1.8692\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [181/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8103\n",
      "Step [189/189]  Loss: 1.8691\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [182/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8102\n",
      "Step [189/189]  Loss: 1.8689\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [183/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8101\n",
      "Step [189/189]  Loss: 1.8688\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7305\n",
      "\n",
      "Epoch [184/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8100\n",
      "Step [189/189]  Loss: 1.8687\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [185/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8099\n",
      "Step [189/189]  Loss: 1.8685\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7309\n",
      "\n",
      "Epoch [186/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8097\n",
      "Step [189/189]  Loss: 1.8684\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7312\n",
      "\n",
      "Epoch [187/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8096\n",
      "Step [189/189]  Loss: 1.8683\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7312\n",
      "\n",
      "Epoch [188/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8095\n",
      "Step [189/189]  Loss: 1.8681\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7312\n",
      "\n",
      "Epoch [189/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8094\n",
      "Step [189/189]  Loss: 1.8680\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7316\n",
      "\n",
      "Epoch [190/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8093\n",
      "Step [189/189]  Loss: 1.8679\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7316\n",
      "\n",
      "Epoch [191/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8092\n",
      "Step [189/189]  Loss: 1.8678\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [192/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8091\n",
      "Step [189/189]  Loss: 1.8676\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [193/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8090\n",
      "Step [189/189]  Loss: 1.8675\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [194/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8089\n",
      "Step [189/189]  Loss: 1.8674\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [195/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8088\n",
      "Step [189/189]  Loss: 1.8673\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [196/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8088\n",
      "Step [189/189]  Loss: 1.8671\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [197/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8087\n",
      "Step [189/189]  Loss: 1.8670\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [198/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8086\n",
      "Step [189/189]  Loss: 1.8669\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [199/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8085\n",
      "Step [189/189]  Loss: 1.8668\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [200/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8084\n",
      "Step [189/189]  Loss: 1.8666\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [201/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8083\n",
      "Step [189/189]  Loss: 1.8665\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [202/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8082\n",
      "Step [189/189]  Loss: 1.8664\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7324\n",
      "\n",
      "Epoch [203/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8081\n",
      "Step [189/189]  Loss: 1.8663\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7324\n",
      "\n",
      "Epoch [204/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8080\n",
      "Step [189/189]  Loss: 1.8662\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [205/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8080\n",
      "Step [189/189]  Loss: 1.8661\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [206/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8079\n",
      "Step [189/189]  Loss: 1.8659\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7316\n",
      "\n",
      "Epoch [207/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8078\n",
      "Step [189/189]  Loss: 1.8658\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7316\n",
      "\n",
      "Epoch [208/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8077\n",
      "Step [189/189]  Loss: 1.8657\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7316\n",
      "\n",
      "Epoch [209/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8076\n",
      "Step [189/189]  Loss: 1.8656\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7316\n",
      "\n",
      "Epoch [210/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8075\n",
      "Step [189/189]  Loss: 1.8655\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7316\n",
      "\n",
      "Epoch [211/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8075\n",
      "Step [189/189]  Loss: 1.8654\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7316\n",
      "\n",
      "Epoch [212/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8074\n",
      "Step [189/189]  Loss: 1.8653\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7316\n",
      "\n",
      "Epoch [213/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8073\n",
      "Step [189/189]  Loss: 1.8652\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7316\n",
      "\n",
      "Epoch [214/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8072\n",
      "Step [189/189]  Loss: 1.8651\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7316\n",
      "\n",
      "Epoch [215/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8071\n",
      "Step [189/189]  Loss: 1.8649\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [216/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8071\n",
      "Step [189/189]  Loss: 1.8648\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [217/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8070\n",
      "Step [189/189]  Loss: 1.8647\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [218/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8069\n",
      "Step [189/189]  Loss: 1.8646\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [219/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8069\n",
      "Step [189/189]  Loss: 1.8645\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [220/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8068\n",
      "Step [189/189]  Loss: 1.8644\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7316\n",
      "\n",
      "Epoch [221/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8067\n",
      "Step [189/189]  Loss: 1.8643\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7316\n",
      "\n",
      "Epoch [222/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8066\n",
      "Step [189/189]  Loss: 1.8642\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [223/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8066\n",
      "Step [189/189]  Loss: 1.8641\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [224/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8065\n",
      "Step [189/189]  Loss: 1.8640\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [225/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8064\n",
      "Step [189/189]  Loss: 1.8639\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7324\n",
      "\n",
      "Epoch [226/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8064\n",
      "Step [189/189]  Loss: 1.8638\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7324\n",
      "\n",
      "Epoch [227/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8063\n",
      "Step [189/189]  Loss: 1.8637\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7324\n",
      "\n",
      "Epoch [228/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8062\n",
      "Step [189/189]  Loss: 1.8636\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7324\n",
      "\n",
      "Epoch [229/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8062\n",
      "Step [189/189]  Loss: 1.8635\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7324\n",
      "\n",
      "Epoch [230/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8061\n",
      "Step [189/189]  Loss: 1.8634\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7324\n",
      "\n",
      "Epoch [231/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8060\n",
      "Step [189/189]  Loss: 1.8633\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7324\n",
      "\n",
      "Epoch [232/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8060\n",
      "Step [189/189]  Loss: 1.8632\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7324\n",
      "\n",
      "Epoch [233/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8059\n",
      "Step [189/189]  Loss: 1.8631\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7324\n",
      "\n",
      "Epoch [234/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8058\n",
      "Step [189/189]  Loss: 1.8630\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7324\n",
      "\n",
      "Epoch [235/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8058\n",
      "Step [189/189]  Loss: 1.8629\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7324\n",
      "\n",
      "Epoch [236/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8057\n",
      "Step [189/189]  Loss: 1.8628\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7324\n",
      "\n",
      "Epoch [237/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8057\n",
      "Step [189/189]  Loss: 1.8627\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7324\n",
      "\n",
      "Epoch [238/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8056\n",
      "Step [189/189]  Loss: 1.8626\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7324\n",
      "\n",
      "Epoch [239/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8055\n",
      "Step [189/189]  Loss: 1.8625\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [240/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8055\n",
      "Step [189/189]  Loss: 1.8624\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7316\n",
      "\n",
      "Epoch [241/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8054\n",
      "Step [189/189]  Loss: 1.8624\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7316\n",
      "\n",
      "Epoch [242/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8054\n",
      "Step [189/189]  Loss: 1.8623\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [243/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8053\n",
      "Step [189/189]  Loss: 1.8622\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [244/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8052\n",
      "Step [189/189]  Loss: 1.8621\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [245/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8052\n",
      "Step [189/189]  Loss: 1.8620\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [246/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8051\n",
      "Step [189/189]  Loss: 1.8619\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [247/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8051\n",
      "Step [189/189]  Loss: 1.8618\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7320\n",
      "\n",
      "Epoch [248/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8050\n",
      "Step [189/189]  Loss: 1.8617\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7324\n",
      "\n",
      "Epoch [249/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8049\n",
      "Step [189/189]  Loss: 1.8616\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7324\n",
      "\n",
      "Epoch [250/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8049\n",
      "Step [189/189]  Loss: 1.8615\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7328\n",
      "\n",
      "Epoch [251/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8048\n",
      "Step [189/189]  Loss: 1.8615\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7328\n",
      "\n",
      "Epoch [252/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8048\n",
      "Step [189/189]  Loss: 1.8614\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7328\n",
      "\n",
      "Epoch [253/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8047\n",
      "Step [189/189]  Loss: 1.8613\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7328\n",
      "\n",
      "Epoch [254/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8046\n",
      "Step [189/189]  Loss: 1.8612\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7328\n",
      "\n",
      "Epoch [255/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8046\n",
      "Step [189/189]  Loss: 1.8611\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7328\n",
      "\n",
      "Epoch [256/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8045\n",
      "Step [189/189]  Loss: 1.8610\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7332\n",
      "\n",
      "Epoch [257/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8045\n",
      "Step [189/189]  Loss: 1.8609\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7332\n",
      "\n",
      "Epoch [258/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8044\n",
      "Step [189/189]  Loss: 1.8609\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7332\n",
      "\n",
      "Epoch [259/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8043\n",
      "Step [189/189]  Loss: 1.8608\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7336\n",
      "\n",
      "Epoch [260/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8043\n",
      "Step [189/189]  Loss: 1.8607\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7336\n",
      "\n",
      "Epoch [261/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8042\n",
      "Step [189/189]  Loss: 1.8606\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7336\n",
      "\n",
      "Epoch [262/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8042\n",
      "Step [189/189]  Loss: 1.8605\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7336\n",
      "\n",
      "Epoch [263/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8041\n",
      "Step [189/189]  Loss: 1.8604\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7332\n",
      "\n",
      "Epoch [264/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8040\n",
      "Step [189/189]  Loss: 1.8604\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7332\n",
      "\n",
      "Epoch [265/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8040\n",
      "Step [189/189]  Loss: 1.8603\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7332\n",
      "\n",
      "Epoch [266/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8039\n",
      "Step [189/189]  Loss: 1.8602\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7336\n",
      "\n",
      "Epoch [267/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8039\n",
      "Step [189/189]  Loss: 1.8601\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7336\n",
      "\n",
      "Epoch [268/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8038\n",
      "Step [189/189]  Loss: 1.8600\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7336\n",
      "\n",
      "Epoch [269/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8038\n",
      "Step [189/189]  Loss: 1.8600\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7336\n",
      "\n",
      "Epoch [270/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8037\n",
      "Step [189/189]  Loss: 1.8599\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7336\n",
      "\n",
      "Epoch [271/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8036\n",
      "Step [189/189]  Loss: 1.8598\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7336\n",
      "\n",
      "Epoch [272/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8036\n",
      "Step [189/189]  Loss: 1.8597\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7332\n",
      "\n",
      "Epoch [273/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8035\n",
      "Step [189/189]  Loss: 1.8596\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7332\n",
      "\n",
      "Epoch [274/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8035\n",
      "Step [189/189]  Loss: 1.8596\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7332\n",
      "\n",
      "Epoch [275/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8034\n",
      "Step [189/189]  Loss: 1.8595\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7332\n",
      "\n",
      "Epoch [276/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8034\n",
      "Step [189/189]  Loss: 1.8594\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7332\n",
      "\n",
      "Epoch [277/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8033\n",
      "Step [189/189]  Loss: 1.8593\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7332\n",
      "\n",
      "Epoch [278/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8032\n",
      "Step [189/189]  Loss: 1.8593\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7332\n",
      "\n",
      "Epoch [279/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8032\n",
      "Step [189/189]  Loss: 1.8592\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7336\n",
      "\n",
      "Epoch [280/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8031\n",
      "Step [189/189]  Loss: 1.8591\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7336\n",
      "\n",
      "Epoch [281/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8031\n",
      "Step [189/189]  Loss: 1.8590\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7340\n",
      "\n",
      "Epoch [282/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8030\n",
      "Step [189/189]  Loss: 1.8590\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7340\n",
      "\n",
      "Epoch [283/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8029\n",
      "Step [189/189]  Loss: 1.8589\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7340\n",
      "\n",
      "Epoch [284/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8029\n",
      "Step [189/189]  Loss: 1.8588\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7340\n",
      "\n",
      "Epoch [285/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8028\n",
      "Step [189/189]  Loss: 1.8587\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7340\n",
      "\n",
      "Epoch [286/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8028\n",
      "Step [189/189]  Loss: 1.8587\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7340\n",
      "\n",
      "Epoch [287/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8027\n",
      "Step [189/189]  Loss: 1.8586\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7340\n",
      "\n",
      "Epoch [288/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8027\n",
      "Step [189/189]  Loss: 1.8585\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7340\n",
      "\n",
      "Epoch [289/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8026\n",
      "Step [189/189]  Loss: 1.8585\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7340\n",
      "\n",
      "Epoch [290/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8025\n",
      "Step [189/189]  Loss: 1.8584\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7340\n",
      "\n",
      "Epoch [291/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8025\n",
      "Step [189/189]  Loss: 1.8583\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7340\n",
      "\n",
      "Epoch [292/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8024\n",
      "Step [189/189]  Loss: 1.8582\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7340\n",
      "\n",
      "Epoch [293/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8024\n",
      "Step [189/189]  Loss: 1.8582\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7344\n",
      "\n",
      "Epoch [294/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8023\n",
      "Step [189/189]  Loss: 1.8581\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7344\n",
      "\n",
      "Epoch [295/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8022\n",
      "Step [189/189]  Loss: 1.8580\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7344\n",
      "\n",
      "Epoch [296/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8022\n",
      "Step [189/189]  Loss: 1.8580\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7340\n",
      "\n",
      "Epoch [297/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8021\n",
      "Step [189/189]  Loss: 1.8579\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7340\n",
      "\n",
      "Epoch [298/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8021\n",
      "Step [189/189]  Loss: 1.8578\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7340\n",
      "\n",
      "Epoch [299/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8020\n",
      "Step [189/189]  Loss: 1.8578\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7340\n",
      "\n",
      "Epoch [300/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8020\n",
      "Step [189/189]  Loss: 1.8577\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7340\n",
      "\n",
      "Epoch [301/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8019\n",
      "Step [189/189]  Loss: 1.8576\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7340\n",
      "\n",
      "Epoch [302/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8018\n",
      "Step [189/189]  Loss: 1.8576\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7348\n",
      "\n",
      "Epoch [303/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8018\n",
      "Step [189/189]  Loss: 1.8575\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7348\n",
      "\n",
      "Epoch [304/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8017\n",
      "Step [189/189]  Loss: 1.8574\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7348\n",
      "\n",
      "Epoch [305/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8017\n",
      "Step [189/189]  Loss: 1.8574\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7348\n",
      "\n",
      "Epoch [306/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8016\n",
      "Step [189/189]  Loss: 1.8573\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7348\n",
      "\n",
      "Epoch [307/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8016\n",
      "Step [189/189]  Loss: 1.8572\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7352\n",
      "\n",
      "Epoch [308/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8015\n",
      "Step [189/189]  Loss: 1.8572\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7352\n",
      "\n",
      "Epoch [309/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8015\n",
      "Step [189/189]  Loss: 1.8571\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [310/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8014\n",
      "Step [189/189]  Loss: 1.8570\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [311/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8013\n",
      "Step [189/189]  Loss: 1.8570\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [312/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8013\n",
      "Step [189/189]  Loss: 1.8569\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [313/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8012\n",
      "Step [189/189]  Loss: 1.8568\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [314/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8012\n",
      "Step [189/189]  Loss: 1.8568\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7352\n",
      "\n",
      "Epoch [315/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8011\n",
      "Step [189/189]  Loss: 1.8567\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7352\n",
      "\n",
      "Epoch [316/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8011\n",
      "Step [189/189]  Loss: 1.8566\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [317/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8010\n",
      "Step [189/189]  Loss: 1.8566\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [318/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8010\n",
      "Step [189/189]  Loss: 1.8565\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [319/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8009\n",
      "Step [189/189]  Loss: 1.8565\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [320/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8009\n",
      "Step [189/189]  Loss: 1.8564\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [321/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8008\n",
      "Step [189/189]  Loss: 1.8563\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [322/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8007\n",
      "Step [189/189]  Loss: 1.8563\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [323/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8007\n",
      "Step [189/189]  Loss: 1.8562\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [324/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8006\n",
      "Step [189/189]  Loss: 1.8561\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7359\n",
      "\n",
      "Epoch [325/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8006\n",
      "Step [189/189]  Loss: 1.8561\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7359\n",
      "\n",
      "Epoch [326/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8005\n",
      "Step [189/189]  Loss: 1.8560\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7359\n",
      "\n",
      "Epoch [327/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8005\n",
      "Step [189/189]  Loss: 1.8560\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7359\n",
      "\n",
      "Epoch [328/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8004\n",
      "Step [189/189]  Loss: 1.8559\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7359\n",
      "\n",
      "Epoch [329/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8004\n",
      "Step [189/189]  Loss: 1.8558\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7359\n",
      "\n",
      "Epoch [330/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8003\n",
      "Step [189/189]  Loss: 1.8558\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7363\n",
      "\n",
      "Epoch [331/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8003\n",
      "Step [189/189]  Loss: 1.8557\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7363\n",
      "\n",
      "Epoch [332/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8002\n",
      "Step [189/189]  Loss: 1.8557\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7363\n",
      "\n",
      "Epoch [333/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8002\n",
      "Step [189/189]  Loss: 1.8556\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7363\n",
      "\n",
      "Epoch [334/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8001\n",
      "Step [189/189]  Loss: 1.8555\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7363\n",
      "\n",
      "Epoch [335/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8001\n",
      "Step [189/189]  Loss: 1.8555\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7363\n",
      "\n",
      "Epoch [336/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8000\n",
      "Step [189/189]  Loss: 1.8554\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7359\n",
      "\n",
      "Epoch [337/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.8000\n",
      "Step [189/189]  Loss: 1.8554\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7359\n",
      "\n",
      "Epoch [338/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7999\n",
      "Step [189/189]  Loss: 1.8553\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7359\n",
      "\n",
      "Epoch [339/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7999\n",
      "Step [189/189]  Loss: 1.8553\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7359\n",
      "\n",
      "Epoch [340/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7998\n",
      "Step [189/189]  Loss: 1.8552\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [341/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7998\n",
      "Step [189/189]  Loss: 1.8551\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [342/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7997\n",
      "Step [189/189]  Loss: 1.8551\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [343/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7997\n",
      "Step [189/189]  Loss: 1.8550\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [344/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7996\n",
      "Step [189/189]  Loss: 1.8550\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [345/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7996\n",
      "Step [189/189]  Loss: 1.8549\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [346/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7995\n",
      "Step [189/189]  Loss: 1.8549\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [347/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7995\n",
      "Step [189/189]  Loss: 1.8548\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7352\n",
      "\n",
      "Epoch [348/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7994\n",
      "Step [189/189]  Loss: 1.8548\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7352\n",
      "\n",
      "Epoch [349/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7994\n",
      "Step [189/189]  Loss: 1.8547\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7348\n",
      "\n",
      "Epoch [350/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7993\n",
      "Step [189/189]  Loss: 1.8546\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7348\n",
      "\n",
      "Epoch [351/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7993\n",
      "Step [189/189]  Loss: 1.8546\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7344\n",
      "\n",
      "Epoch [352/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7992\n",
      "Step [189/189]  Loss: 1.8545\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7344\n",
      "\n",
      "Epoch [353/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7992\n",
      "Step [189/189]  Loss: 1.8545\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7344\n",
      "\n",
      "Epoch [354/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7991\n",
      "Step [189/189]  Loss: 1.8544\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7344\n",
      "\n",
      "Epoch [355/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7991\n",
      "Step [189/189]  Loss: 1.8544\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7344\n",
      "\n",
      "Epoch [356/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7990\n",
      "Step [189/189]  Loss: 1.8543\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7344\n",
      "\n",
      "Epoch [357/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7990\n",
      "Step [189/189]  Loss: 1.8543\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7344\n",
      "\n",
      "Epoch [358/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7989\n",
      "Step [189/189]  Loss: 1.8542\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7348\n",
      "\n",
      "Epoch [359/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7989\n",
      "Step [189/189]  Loss: 1.8542\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7348\n",
      "\n",
      "Epoch [360/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7988\n",
      "Step [189/189]  Loss: 1.8541\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7352\n",
      "\n",
      "Epoch [361/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7988\n",
      "Step [189/189]  Loss: 1.8541\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7352\n",
      "\n",
      "Epoch [362/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7987\n",
      "Step [189/189]  Loss: 1.8540\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7352\n",
      "\n",
      "Epoch [363/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7987\n",
      "Step [189/189]  Loss: 1.8540\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7352\n",
      "\n",
      "Epoch [364/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7986\n",
      "Step [189/189]  Loss: 1.8539\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7352\n",
      "\n",
      "Epoch [365/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7986\n",
      "Step [189/189]  Loss: 1.8539\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7352\n",
      "\n",
      "Epoch [366/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7985\n",
      "Step [189/189]  Loss: 1.8538\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7352\n",
      "\n",
      "Epoch [367/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7985\n",
      "Step [189/189]  Loss: 1.8537\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7352\n",
      "\n",
      "Epoch [368/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7984\n",
      "Step [189/189]  Loss: 1.8537\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7352\n",
      "\n",
      "Epoch [369/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7984\n",
      "Step [189/189]  Loss: 1.8536\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7352\n",
      "\n",
      "Epoch [370/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7983\n",
      "Step [189/189]  Loss: 1.8536\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7352\n",
      "\n",
      "Epoch [371/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7983\n",
      "Step [189/189]  Loss: 1.8535\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [372/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7982\n",
      "Step [189/189]  Loss: 1.8535\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [373/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7982\n",
      "Step [189/189]  Loss: 1.8534\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [374/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7981\n",
      "Step [189/189]  Loss: 1.8534\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [375/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7981\n",
      "Step [189/189]  Loss: 1.8533\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [376/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7980\n",
      "Step [189/189]  Loss: 1.8533\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [377/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7980\n",
      "Step [189/189]  Loss: 1.8532\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [378/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7979\n",
      "Step [189/189]  Loss: 1.8532\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [379/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7979\n",
      "Step [189/189]  Loss: 1.8532\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Epoch [380/1000]\n",
      "--------------------------------\n",
      "########### Training ###########\n",
      "Step [001/189]  Loss: 1.7978\n",
      "Step [189/189]  Loss: 1.8531\n",
      "########## Validation ##########\n",
      "Average accuracy over validation set:  0.7355\n",
      "\n",
      "Early stopping initiated!\n"
     ]
    }
   ],
   "source": [
    "model = ClassificationAverageModel(embedding, embed_size, len(labels))\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses, val_accuracies = train(train_loader, val_loader, model, loss, optimizer, len(labels), EPOCHS, DEVICE, N_EARLY_STOPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T14:36:51.647711Z",
     "iopub.status.busy": "2024-01-07T14:36:51.647003Z",
     "iopub.status.idle": "2024-01-07T14:36:51.715448Z",
     "shell.execute_reply": "2024-01-07T14:36:51.714912Z",
     "shell.execute_reply.started": "2024-01-07T14:36:51.647654Z"
    }
   },
   "outputs": [],
   "source": [
    "# add model graph to tensorboard\n",
    "\n",
    "writer = SummaryWriter('runs/model_graph')\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "text, _ = next(dataiter)\n",
    "writer.add_graph(best_model, torch.squeeze(text))\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T15:18:24.128257Z",
     "iopub.status.busy": "2024-01-07T15:18:24.127975Z",
     "iopub.status.idle": "2024-01-07T15:18:24.135538Z",
     "shell.execute_reply": "2024-01-07T15:18:24.134781Z",
     "shell.execute_reply.started": "2024-01-07T15:18:24.128236Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss, n_classes, device):\n",
    "\n",
    "    print('########### Testing ###########')\n",
    "    model.eval()\n",
    "    current_accuracies = []\n",
    "    current_losses = []\n",
    "\n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        x = torch.squeeze(x).to(device)\n",
    "        y = torch.squeeze(to_one_hot(y, n_classes)).to(device)\n",
    "\n",
    "        logits = model.forward(x)\n",
    "        \n",
    "        batch_accuracy = accuracy(logits.detach().cpu().numpy(), y.detach().cpu().numpy())\n",
    "        batch_loss = loss(logits, y)\n",
    "        \n",
    "        current_losses.append(batch_loss.detach().cpu().numpy())\n",
    "        current_accuracies.append(batch_accuracy)\n",
    "\n",
    "    avg_accuracy = np.sum(current_accuracies) / len(current_accuracies)\n",
    "    avg_loss = np.sum(current_losses) / len(current_losses)\n",
    "\n",
    "    print('Average accuracy over test set:\\t{:.4f}'.format((avg_accuracy)))\n",
    "    print('Average loss over test set:\\t{:.4f}\\n'.format((avg_loss)))\n",
    "\n",
    "    return current_losses, current_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-07T15:18:24.519435Z",
     "iopub.status.busy": "2024-01-07T15:18:24.517810Z",
     "iopub.status.idle": "2024-01-07T15:18:24.576057Z",
     "shell.execute_reply": "2024-01-07T15:18:24.575553Z",
     "shell.execute_reply.started": "2024-01-07T15:18:24.519379Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Testing ###########\n",
      "Average accuracy over test set:\t0.7387\n",
      "Average loss over test set:\t1.8880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model = ClassificationAverageModel(embedding, embed_size, len(labels))\n",
    "best_model.to(DEVICE)\n",
    "best_model.load_state_dict(torch.load('./model_saves/model.pt'))\n",
    "\n",
    "test_loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "test_losses, test_accuracies = test(test_loader, best_model, test_loss_func, len(labels), DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Document Classification using DistilBERT (Huggingface)\n",
    "\n",
    "In this section, I use a transformer-based model—**DistilBERT**—to classify the same documents. DistilBERT is a smaller, faster version of BERT and is well-suited for tasks with limited resources.\n",
    "\n",
    "Instead of using manually averaged word embeddings, this model learns contextualized embeddings directly from subword tokens. The `[CLS]` token representation is extracted for classification, followed by a dropout and a linear layer.\n",
    "\n",
    "This section highlights the power of transformers in comparison to traditional embedding-based models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We picked the DistilBert instead of the normal Bert to get faster training. \n",
    "#DistilBert resulted in decent performance anyways.\n",
    "\n",
    "model_name = 'distilbert-base-uncased'\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "distil_bert_model = DistilBertModel.from_pretrained(model_name)\n",
    "\n",
    "#Just familiarizing ourselved with the BERT's structure.\n",
    "get_vocab = tokenizer.get_vocab()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We decided to implement classes for the purpose of tokenization. This seemed to be a very convenient way. \n",
    "\n",
    "class TokenizingDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_doc_len=50):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_doc_len = max_doc_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    #So, basically we use the tokenizer, the encoder from the transformers library\n",
    "    #it outputs input_ids and an attention mask, both of them being of utmost importance for the BERT model\n",
    "    #additionally, we also get the labels in correct tensor shape, making the dataset ready for training\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encodings = self.tokenizer.encode_plus(\n",
    "            self.texts[index],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_doc_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encodings['input_ids'].squeeze()\n",
    "        attention_mask = encodings['attention_mask'].squeeze()\n",
    "        labels = torch.tensor(self.labels[index], dtype=torch.long)\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDistilBERTModel(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        \n",
    "        super(ClassificationDistilBERTModel, self).__init__()\n",
    "        \n",
    "        self.distilbert = distil_bert_model\n",
    "        \n",
    "        #it's crucial that we use a linear transfromation before the final classifier \n",
    "        #the output has the shape of the BERT model \n",
    "        #and is mapped via the next linear transformation to the number of classes \n",
    "        #the dropout probability is used to make sure that the model is not overfitting on the training data \n",
    "        \n",
    "        self.transform_layer = nn.Linear(self.distilbert.config.dim, self.distilbert.config.dim)\n",
    "        self.output_classfier = nn.Linear(self.distilbert.config.dim, n_classes)\n",
    "        self.regularizer = nn.Dropout(p = 0.2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None,  return_probs=False):\n",
    "        \n",
    "        model_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        #we are retaining the output of the model\n",
    "        #consequently the tokens, the features in other words are extracted \n",
    "        \n",
    "        \n",
    "        features_extract = model_output[0][:, 0] \n",
    "        transformed = self.transform_layer(features_extract)  \n",
    "        \n",
    "        #we also decided to make sure there is some non-linearity involved, \n",
    "        #since our model performed terrible at the beginning\n",
    "        \n",
    "        activated = nn.ReLU()(transformed)  \n",
    "        regularized_features = self.regularizer(activated)  \n",
    "        \n",
    "        logits =  self.output_classfier(regularized_features)  \n",
    "        \n",
    "        if return_probs:\n",
    "            #whenever asked the probabilities can be returned\n",
    "            #the softmax function easily translates the logits into probabilities \n",
    "            softmax_transform = F.softmax(logits, dim=-1)\n",
    "            return softmax_transform\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the BERT model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_BERT(train_loader, val_loader, model, loss, optimizer, epochs, device, progress_every=10, n_early_stop=50, model_name='bert model'):\n",
    "    \n",
    "    best_val_accuracy = 0\n",
    "    n_not_improved = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        current_loss = 0.0\n",
    "        \n",
    "        correct_pred = 0\n",
    "        nr_of_pred = 0\n",
    "        \n",
    "        tracking_progress = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Starting Epoch {}\".format(epoch+1))\n",
    "        \n",
    "        #So, we go through all batches, most of the training process is 1:1 the same as in Task A \n",
    "        #however, we do the batching, the loss, and accuracy computation a bit differently here\n",
    "        \n",
    "        for i, batch in tracking_progress:\n",
    "            \n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logits = model(**batch)\n",
    "            y = batch['labels']\n",
    "            batch_loss = loss(logits, y)\n",
    "            \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            current_loss += batch_loss.item()\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "            \n",
    "            #for the accuracies \n",
    "            \n",
    "            correct_pred += (predicted == batch['labels']).sum().item()\n",
    "            nr_of_pred += batch['labels'].size(0)\n",
    "            \n",
    "            if (i + 1) % progress_every == 0:\n",
    "                tracking_progress.set_postfix({\n",
    "                    'Loss at the moment': current_loss / (i + 1),\n",
    "                    'Accuracy at the moment': correct_pred / nr_of_pred\n",
    "                })\n",
    "        \n",
    "    \n",
    "        avg_loss = current_loss / len(train_loader)\n",
    "        avg_accuracy = correct_pred / nr_of_pred\n",
    "        \n",
    "        tqdm.write(f'Epoch {epoch+1} ended, Average Loss during training: {avg_loss}, Average Accuracy during training: {avg_accuracy}')\n",
    "\n",
    "        val_accuracy = model_eval(model, val_loader)\n",
    "        tqdm.write(f'Average accuracy over validation set: {val_accuracy}')\n",
    "        \n",
    "        #Just keeping the best performing model \n",
    "        \n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            n_not_improved = 0\n",
    "            torch.save(model.state_dict(), f'./model_saves/{model_name}.pt')\n",
    "            tqdm.write('Out of all configuration the model was saved as the best model till now.')\n",
    "            \n",
    "        else:\n",
    "            n_not_improved += 1\n",
    "            if n_not_improved >= n_early_stop:\n",
    "                tqdm.write('Early stopping initiated!')\n",
    "                break\n",
    "        \n",
    "        #Early stopping is crucial for avoiding overfitting \n",
    "        \n",
    "\n",
    "#this function is mainly used for the validation set \n",
    "#so wehenever one epoch is finished it is evaluated via this method \n",
    "                \n",
    "\n",
    "def model_eval(current_model, dataloader):\n",
    "    current_model.eval()\n",
    "    \n",
    "    predicted_labels = []\n",
    "    true_labels = []\n",
    "    \n",
    "    tracking_progress = tqdm(dataloader, desc='Starting the evaluation:')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tracking_progress:\n",
    "            \n",
    "            batch = {key: value.to(DEVICE) for key, value in batch.items()}\n",
    "            \n",
    "            #here we use the probabilties \n",
    "            \n",
    "            probs = model(**batch, return_probs=True)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "            \n",
    "            predicted_labels.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(batch['labels'].cpu().numpy())\n",
    "            \n",
    "    calc_accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    \n",
    "    return calc_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the model, the optimizer and the loss function of our interest\n",
    "#We tried out several learning rates to enhance performance\n",
    "\n",
    "model = ClassificationDistilBERTModel(n_classes=len(labels))\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  \n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_data['text'].tolist()\n",
    "train_labels = train_data['labels'].tolist()\n",
    "val_texts = val_data['text'].tolist()\n",
    "val_labels = val_data['labels'].tolist()\n",
    "test_texts = test_data['text'].tolist()\n",
    "test_labels = test_data['labels'].tolist()\n",
    "\n",
    "train_dataset_bert = TokenizingDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset_bert = TokenizingDataset(val_texts, val_labels, tokenizer)\n",
    "test_dataset_bert = TokenizingDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "TRAIN_LOADER = DataLoader(train_dataset_bert, batch_size=16, shuffle=True)\n",
    "VAL_LOADER = DataLoader(val_dataset_bert, batch_size=16)\n",
    "TEST_LOADER = DataLoader(test_dataset_bert, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The real training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c6b996740c74f9195ceab087c15f546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting Epoch 1:   0%|          | 0/757 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ended, Average Loss during training: 0.6185271241007031, Average Accuracy during training: 0.8336085879438481\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c905cd5bb0b143faa25704dcf1f83df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting the evaluation::   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy over validation set: 0.8124036979969184\n",
      "The model performing the best until now was saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87da2859476a4c459af501f0f2123955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting Epoch 2:   0%|          | 0/757 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 ended, Average Loss during training: 0.4149468993987086, Average Accuracy during training: 0.8844756399669694\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a07468e4a94014b8b07a9149f1e280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting the evaluation::   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy over validation set: 0.7981510015408321\n"
     ]
    }
   ],
   "source": [
    "train_BERT(TRAIN_LOADER, VAL_LOADER, model, loss, optimizer, epochs=2, device=DEVICE, progress_every=5, n_early_stop=50, model_name='bert_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_accuracies(data_loader, model_saves_path, device):\n",
    "    # Load the best model\n",
    "    model =  ClassificationDistilBERTModel(n_classes=len(labels))  \n",
    "    model.load_state_dict(torch.load(model_saves_path))\n",
    "    model.to(device)\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    accuracy = model_eval(model, data_loader)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e9f217d4ef45bd85fd83d6b82a8a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting the evaluation::   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model achieved the following accuracy on the test set 0.8196531791907514\n"
     ]
    }
   ],
   "source": [
    "#Getting the accuracy of the best model on the test set\n",
    "\n",
    "path_to_the_best_model = './model_saves/bert_model.pt'\n",
    "test_accuracy = getting_accuracies(TEST_LOADER, path_to_the_best_model, DEVICE)\n",
    "\n",
    "print('Our model achieved the following accuracy on the test set {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0609c71f58d246ea98a61e97a83dd426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Starting the evaluation::   0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our  best performing model achieved the following accuracy on the validation set 0.811633281972265\n"
     ]
    }
   ],
   "source": [
    "#Getting the accuracy of the best model on the validation set\n",
    "\n",
    "path_to_the_best_model = './model_saves/bert_model.pt'\n",
    "validation_accuracy = getting_accuracies(VAL_LOADER, path_to_the_best_model, DEVICE)\n",
    "\n",
    "print('Our  best performing model achieved the following accuracy on the validation set {}'.format(validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test set accuracy  Validation set accuracy\n",
      "          0.819653                 0.811633\n"
     ]
    }
   ],
   "source": [
    "data = {'Test set accuracy': [test_accuracy],\n",
    "        'Validation set accuracy': [validation_accuracy]}\n",
    "table = pd.DataFrame(data)\n",
    "\n",
    "table = table.to_string(index=False)\n",
    "\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
